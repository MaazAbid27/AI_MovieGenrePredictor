{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkndMKVWVr4M"
      },
      "source": [
        "### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7iGcNrkSnOn",
        "outputId": "79d250f7-ee85-4e3c-f93a-1646becadc3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "98/98 [==============================] - 65s 577ms/step - loss: 0.2911 - accuracy: 0.0472 - val_loss: 0.2146 - val_accuracy: 0.0558\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 48s 487ms/step - loss: 0.2149 - accuracy: 0.0582 - val_loss: 0.2141 - val_accuracy: 0.0558\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 47s 478ms/step - loss: 0.2146 - accuracy: 0.0593 - val_loss: 0.2141 - val_accuracy: 0.0558\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 47s 482ms/step - loss: 0.2138 - accuracy: 0.0586 - val_loss: 0.2132 - val_accuracy: 0.0558\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - 48s 490ms/step - loss: 0.2118 - accuracy: 0.0675 - val_loss: 0.2127 - val_accuracy: 0.0531\n",
            "Epoch 6/20\n",
            "98/98 [==============================] - 45s 457ms/step - loss: 0.2089 - accuracy: 0.0710 - val_loss: 0.2106 - val_accuracy: 0.0650\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - 52s 528ms/step - loss: 0.2052 - accuracy: 0.0785 - val_loss: 0.2086 - val_accuracy: 0.0640\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - 44s 454ms/step - loss: 0.2010 - accuracy: 0.0962 - val_loss: 0.2057 - val_accuracy: 0.0897\n",
            "Epoch 9/20\n",
            "98/98 [==============================] - 47s 478ms/step - loss: 0.1953 - accuracy: 0.1000 - val_loss: 0.2086 - val_accuracy: 0.0897\n",
            "Epoch 10/20\n",
            "98/98 [==============================] - 48s 488ms/step - loss: 0.1909 - accuracy: 0.1181 - val_loss: 0.1977 - val_accuracy: 0.1025\n",
            "Epoch 11/20\n",
            "98/98 [==============================] - 46s 475ms/step - loss: 0.1834 - accuracy: 0.1337 - val_loss: 0.1924 - val_accuracy: 0.1016\n",
            "Epoch 12/20\n",
            "98/98 [==============================] - 45s 461ms/step - loss: 0.1781 - accuracy: 0.1406 - val_loss: 0.1903 - val_accuracy: 0.1144\n",
            "Epoch 13/20\n",
            "98/98 [==============================] - 47s 478ms/step - loss: 0.1731 - accuracy: 0.1614 - val_loss: 0.1917 - val_accuracy: 0.1253\n",
            "Epoch 14/20\n",
            "98/98 [==============================] - 44s 452ms/step - loss: 0.1693 - accuracy: 0.1742 - val_loss: 0.1846 - val_accuracy: 0.1281\n",
            "Epoch 15/20\n",
            "98/98 [==============================] - 48s 493ms/step - loss: 0.1650 - accuracy: 0.1779 - val_loss: 0.1828 - val_accuracy: 0.1446\n",
            "Epoch 16/20\n",
            "98/98 [==============================] - 44s 453ms/step - loss: 0.1617 - accuracy: 0.1864 - val_loss: 0.1820 - val_accuracy: 0.1400\n",
            "Epoch 17/20\n",
            "98/98 [==============================] - 44s 454ms/step - loss: 0.1591 - accuracy: 0.1969 - val_loss: 0.1814 - val_accuracy: 0.1327\n",
            "Epoch 18/20\n",
            "98/98 [==============================] - 47s 478ms/step - loss: 0.1564 - accuracy: 0.1948 - val_loss: 0.1793 - val_accuracy: 0.1656\n",
            "Epoch 19/20\n",
            "98/98 [==============================] - 47s 478ms/step - loss: 0.1539 - accuracy: 0.2173 - val_loss: 0.1786 - val_accuracy: 0.1537\n",
            "Epoch 20/20\n",
            "98/98 [==============================] - 50s 508ms/step - loss: 0.1518 - accuracy: 0.2166 - val_loss: 0.1770 - val_accuracy: 0.1610\n",
            "137/137 [==============================] - 9s 62ms/step\n",
            "35/35 [==============================] - 3s 85ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "Validation Accuracy (Stacked Model): 0.5690759377859104\n",
            "model saved successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Genres:\n",
            "a group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.: \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle\n",
        "\n",
        "# Assuming tokenizer is your Tokenizer object\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/combined_cleaned_dateset.csv')\n",
        "\n",
        "# Preprocess the genres\n",
        "genres = df['genre']\n",
        "genres.fillna('', inplace=True)\n",
        "genres = [genre.split(',') for genre in genres]\n",
        "\n",
        "# Preprocess the descriptions\n",
        "max_words = 1000\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['description'])\n",
        "X = tokenizer.texts_to_sequences(df['description'])\n",
        "X = pad_sequences(X)\n",
        "with open('/content/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(genres)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=52)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=X.shape[1]))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=80, batch_size=45, validation_data=(X_val, y_val))\n",
        "\n",
        "rnn_predictions_train = model.predict(X_train)\n",
        "rnn_predictions_val = model.predict(X_val)\n",
        "\n",
        "test_descriptions = [\"a group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.\"]\n",
        "X_test = tokenizer.texts_to_sequences(test_descriptions)\n",
        "X_test = pad_sequences(X_test, maxlen=X.shape[1])\n",
        "rnn_predictions_test = model.predict(X_test)\n",
        "\n",
        "X_train_with_predictions = np.concatenate((X_train, rnn_predictions_train), axis=1)\n",
        "X_val_with_predictions = np.concatenate((X_val, rnn_predictions_val), axis=1)\n",
        "X_test_with_predictions = np.concatenate((X_test, rnn_predictions_test), axis=1)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
        "rf_model.fit(X_train_with_predictions, y_train)\n",
        "\n",
        "# Evaluate the stacked model\n",
        "accuracy = rf_model.score(X_val_with_predictions, y_val)\n",
        "print(\"Validation Accuracy (Stacked Model):\", accuracy)\n",
        "\n",
        "model.save('/content/mymodel.h5')\n",
        "print(\"model saved successfully.\")\n",
        "\n",
        "# Make predictions using the stacked model\n",
        "stacked_predictions = rf_model.predict(X_test_with_predictions)\n",
        "\n",
        "# Convert predictions to genres\n",
        "threshold = 0.25\n",
        "binary_predictions = np.where(stacked_predictions > threshold, 1, 0)\n",
        "predicted_genres = mlb.inverse_transform(binary_predictions)\n",
        "predicted_genres = [list(set(genres)) for genres in predicted_genres]\n",
        "\n",
        "print(\"Predicted Genres:\")\n",
        "for desc, genres in zip(test_descriptions, predicted_genres):\n",
        "    print(f\"{desc}: {', '.join(genres)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0lnC0WWWnla"
      },
      "source": [
        "###FOR USING AND TESTING THE DATA(UI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "IJpjqGpGyDNG",
        "outputId": "768cf0fd-eaee-43b0-ef31-33830572933f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at /content/mymodel.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23af664b04ba>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/mymodel.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/mymodel.h5"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model('/content/mymodel.h5')\n",
        "with open('/content/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "test_descriptions = []\n",
        "print('enter input: ')\n",
        "inp=str(input())\n",
        "\n",
        "test_descriptions.append(inp)\n",
        "X_test = tokenizer.texts_to_sequences(test_descriptions)\n",
        "X_test = pad_sequences(X_test, maxlen=X.shape[1])\n",
        "\n",
        "predictions = loaded_model.predict(X_test)\n",
        "\n",
        "threshold = 0.3\n",
        "binary_predictions = np.where(predictions > threshold, 1, 0)\n",
        "predicted_genres = mlb.inverse_transform(binary_predictions)\n",
        "predicted_genres = [list(set(genres)) for genres in predicted_genres]\n",
        "\n",
        "print(\"Predicted Genres:\")\n",
        "for desc, genres in zip(test_descriptions, predicted_genres):\n",
        "    print(f\"{desc}: {', '.join(genres)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}